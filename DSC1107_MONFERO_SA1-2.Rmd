---
title: "DSC1107_MONFERO_SA1.2"
author: "John Benedict A. Monfero"
date: "2025-03-15"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### **Objectives**:
> The purpose of this summative assessment is to evaluate students' ability to apply data mining techniques, data visualization, data wrangling, and predictive modeling using R. Students will work with a provided dataset to perform exploratory data analysis, data transformation, model tuning, and regression-based methods.

#### **Instructions**:
1. **Dataset:** `customer_churn.csv`, which contains customer demographics, service usage data, and churn labels.
2. **Tools:** Use R and relevant libraries such as `tidyverse`, `caret`, `glmnet`, and `ggplot2`.

```{r, include=FALSE}
library(tidyverse)
library(stat471) # Creating Correlation Heatmap Values
library(ggcorrplot) # Visualizing Correlation as a Matrix
library(caret)  # For machine learning workflow
library(rpart)  # Decision tree
library(rpart.plot)  # For tree visualization
library(glmnet)  # Logistic regression with regularization
library(randomForest) # For creation of randomForest Model
```

3. **Submission:** Submit an R Markdown (`.Rmd`) file or a Jupyter Notebook (`.ipynb`) containing your code, visualizations, and explanations.

## Unit 1: R for Data Mining
### 1. Intro to Modern Data Mining
> Load the dataset and provide an overview of its structure (e.g., dimensions, missing values, types of variables).

```{r}
# Load out the Dataset
dataset <- read.csv("customer_churn.csv")
```

```{r}
# General Overview of the dataset's structure:
glimpse(dataset)
```

+ *CustomerID* (`chr`) – A unique identifier for each customer. It doesn't have predictive value but is useful for tracking individuals.
+ *Gender* (`chr`) – Indicates whether the customer is "Male" or "Female". This can be used to analyze if gender has an impact on churn.
+ *SeniorCitizen* (`int`) – A binary variable where 1 indicates that the customer is a senior citizen (typically 65+ years old) and 0 means they are not. This helps in understanding if age influences churn behavior.
+ *Partner* (`chr`) – "Yes" if the customer has a spouse or partner, "No" otherwise. This may indicate social stability, which could influence customer loyalty.
+ *Dependents* (`chr`) – "Yes" if the customer has dependents (such as children or elderly family members), "No" otherwise. Customers with dependents may have different service needs and spending behaviors.
+ *Tenure* (`int`) – The number of months the customer has been with the service provider. Longer tenure often correlates with customer loyalty.
+ *PhoneService* (`chr`) – "Yes" if the customer has a phone service, "No" otherwise. This shows whether the customer subscribes to the provider’s phone services.
+ *InternetService* (`chr`) – Type of internet service the customer has, such as "Fiber optic", "DSL", or "No" (if the customer does not have internet service). Different internet service types may affect churn rates.
+ *Contract* (`chr`) – The type of contract the customer has, such as "Month-to-month", "One year", or "Two year". Longer contracts usually indicate customer retention, while month-to-month contracts are more flexible but may lead to higher churn.
+ *MonthlyCharges* (`dbl`) – The amount the customer is billed each month for their services. High monthly charges may lead to churn if customers feel the cost is too high.
+ *TotalCharges* (`dbl`) – The total amount charged to the customer during their tenure. This gives insight into a customer's lifetime value.
+ Churn (`chr`) – The target variable indicating whether the customer has left the service ("Yes") or remains active ("No"). This is the outcome you want to predict or analyze.

```{r}
# Checking missing values on each variables in the dataset
if (sum(is.na(dataset)) == 0){
  print("None observations had any `NA` entries to all variables in concern")
} else {
  print("At least one observations has `NA` entries to any variables in concern")
}
```

> Explain why data mining is important for this dataset.

```{r}
print("According to IBM Technology (2023), Data Mining became a fundamental strategy nowadays; as we can convert any bussiness-driven data into meaningful patterns and guide the grant users to have better basis and desicions about their market and trends they will need. With Data Mining, we perform such processing data, indentifying patterns, and regression lines in the specific amounts of variables and observations provided by our dataset.")

print("To further understand, the dataset `customer_churn.csv` contains 12 variables which appears to determine knowing the customer churn analysis; rather focuses on why customers may want to leave the services offered by a company.")

print("As previously mentioned, Data Mining for the dataset `customer_churn.csv` opens opportunity to create data visualizations, neccesary tuning models, and compiling regression-based methods (from tidy enough or not yet) into meaningful outcomes that can be the reference for desicion making skills within the context concerned.")
```

### 2. Data Visualization

> Create at least three meaningful visualizations to explore relationships in the data (e.g., churn rate by tenure, service type, or monthly charges).

```{r}
# Convert Churn to a factor for better visualization
dataset$Churn <- as.factor(dataset$Churn)
dataset$Contract <- as.factor(dataset$Contract)

# Plot
ggplot(dataset, aes(x = Tenure, fill = Churn)) +
  geom_histogram(binwidth = 10, position = "fill", alpha = 0.8) + 
  facet_wrap(~Contract) +
  labs(title = "Churn Rate by Contract Type and Tenure",
       x = "Tenure (Months)", 
       y = "Proportion of Customers",
       fill = "Churn Status")
```
```{r}
ggplot(dataset, aes(x = MonthlyCharges, y = TotalCharges, color = Churn)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "lm", se = FALSE, color = "black") + 
  labs(title = "Monthly Charges vs Total Charges by Churn Status",
       x = "Monthly Charges ($)",
       y = "Total Charges ($)",
       color = "Churn Status")
```

```{r}
ggplot(dataset, aes(x = InternetService, y = MonthlyCharges, fill = InternetService)) +
  geom_boxplot(alpha = 0.7, outlier.color = "red", outlier.shape = 16) +
  facet_wrap(~ Contract) +  
  labs(title = "Monthly Charges by Internet Service and Contract Type",
       x = "Internet Service Type",
       y = "Monthly Charges ($)",
       fill = "Internet Service")
```

> Provide insights based on the visualizations.

```{r}

```

### 3. Data Transformation

> Handle missing values appropriately.

```{r}
colSums(is.na(dataset))
```

> Convert categorical variables into factor variables.

```{r}
dataset <- dataset %>%
  mutate(
    Gender = as.factor(Gender),
    Partner = as.factor(Partner),
    Dependents = as.factor(Dependents),
    PhoneService = as.factor(PhoneService),
    InternetService = as.factor(InternetService),
    Contract = as.factor(Contract),
    Churn = as.factor(Churn) # VERY IMPORTANT : MAIN TARGET VARIABLE
  ) %>%
  glimpse()
```

> Normalize or standardize numerical features where necessary.

+ Did you know? **Min-Max scaling** is a *normalization technique* that enables us to scale data in a dataset to a specific range using each feature’s `minimum` and `maximum` value.

$x_{\text{normalized}}=\frac{x_i - \min{(x)}}{\max{(x)}-\min{(x)}}$

```{r}
# Create a function that does perform normalization through Min-Max Scaling
normalize <- function(x) {
  return((x - min(x)) / (max(x) - min(x)))
}
```

```{r}
# Normalized the `MonthlyCharges` and `TotalCharges`
dataset <- dataset %>%
  mutate(
    MonthlyCharges = normalize(MonthlyCharges),
    TotalCharges = normalize(TotalCharges)
  )

dataset
```
```{r}
binwidth_fd <- function(x) {
  IQR_x <- IQR(x, na.rm = TRUE)
  n <- length(na.omit(x))  # Exclude NA values
  return(2 * IQR_x / (n^(1/3)))  # Freedman-Diaconis formula
}

# Preview visually the normalized data
ggplot(dataset, aes(x = MonthlyCharges)) +
  geom_histogram(binwidth = binwidth_fd(dataset$MonthlyCharges), fill = "steelblue", color = "black", alpha = 0.7) +
  labs(title = "Histogram of Monthly Charges (Auto Binwidth)")

ggplot(dataset, aes(x = TotalCharges)) +
  geom_histogram(binwidth = binwidth_fd(dataset$TotalCharges), fill = "seagreen", color = "black", alpha = 0.7) +
  labs(title = "Histogram of Total Charges (Auto Binwidth)")
```


### 4. Data Wrangling

> Filter data to remove outliers.

+ Did you know? We can utilize the concept of **IQR (Interquartile Range)** to detect any possible outliers within dataset.
```{r}
remove_outliers <- function(df, column) {
  Q1 <- quantile(df[[column]], 0.25, na.rm = TRUE)
  Q3 <- quantile(df[[column]], 0.75, na.rm = TRUE)
  IQR <- Q3 - Q1
  df %>% filter(df[[column]] >= (Q1 - 1.5 * IQR) & df[[column]] <= (Q3 + 1.5 * IQR))
}

remove_outliers(dataset, "MonthlyCharges")
remove_outliers(dataset, "TotalCharges")
```

> Create new derived variables that may help in predictive modeling.

```{r}
dataset <- dataset %>%
  mutate(TenureCategory = case_when(
    Tenure <= 12 ~ "Short-term",
    Tenure > 12 & Tenure <= 36 ~ "Medium-term",
    TRUE ~ "Long-term"
  ))
```

> Aggregate or summarize data if necessary.

```{r}
print("None")
```

### 5. Review

> Summarize key takeaways from the exploratory data analysis process.


## Unit 2: Tuning Predictive Models
### 6. Model Complexity

> Fit a decision tree and logistic regression model.

```{r}
# Split data into train/test (80/20 split)
set.seed(123)
train_index <- createDataPartition(dataset$Churn, p = 0.8, list = FALSE)
train_data <- dataset[train_index, ]
test_data <- dataset[-train_index, ]

train_data$Churn <- factor(train_data$Churn, levels = c("No", "Yes"))
test_data$Churn <- factor(test_data$Churn, levels = c("No", "Yes"))

# Decision Tree Model
tree_model <- rpart(Churn ~ ., 
                    data = train_data, 
                    method = "class",  # For classification
                    )

#Visualize the Tree
rpart.plot(tree_model, box.palette = "auto", nn = TRUE)

# Prediction
predictions <- predict(tree_model, test_data, type = "class")
confusionMatrix(predictions, test_data$Churn)
```

> Compare their complexities and explain trade-offs.

***
Compare Complexity & Trade-offs
Decision Trees: Can capture non-linear relationships, but risk overfitting if not pruned.
Logistic Regression: Assumes linear relationships and is more interpretable, but may underfit complex patterns.
***

### 7. Bias-Variance Trade-Off

> Explain the concept of bias-variance trade-off in the context of the models trained.

Concept Explanation
Bias: Error due to overly simplistic models (e.g., linear regression).
Variance: Error due to overly complex models that memorize data (e.g., deep trees).
Trade-off: A balance is needed—too simple → underfitting, too complex → overfitting.
Effect of Model Complexity
Decision Tree: High variance, but can be pruned to reduce overfitting.
Logistic Regression: Higher bias but lower variance.

> Discuss how model complexity affects performance.

Effect of Model Complexity
Decision Tree: High variance, but can be pruned to reduce overfitting.
Logistic Regression: Higher bias but lower variance.

**Visualizing Overfitting in Decision Trees**
```{r}
pruned_tree <- prune(tree_model, cp = tree_model$cptable[which.min(tree_model$cptable[, "xerror"]), "CP"])
rpart.plot(pruned_tree)
```

### 8. Cross-Validation

> Use k-fold cross-validation (k=10) to evaluate model performance.

```{r}
set.seed(42)
control <- trainControl(method = "cv", number = 10)  # 10-fold CV

# Logistic Regression CV
log_cv <- train(Churn ~ ., data = trainData, method = "glm", family = "binomial", trControl = control)

# Decision Tree CV
tree_cv <- train(Churn ~ ., data = trainData, method = "rpart", trControl = control)

# Compare Performance
log_cv$results
tree_cv$results
```

> Report and interpret accuracy, precision, recall, and F1-score.

### 9. Classification

> Train a Random Forest classifier to predict customer churn.

> Tune hyperparameters using grid search.

> Report final model performance.

## Unit 3: Regression-Based Methods
### 10. Logistic Regression

> Fit a logistic regression model using Churn as the dependent variable and Tenure, MonthlyCharges, and TotalCharges as independent variables.

> Interpret the coefficients and assess model significance using p-values.

### 11. Regression in High Dimensions

> Discuss the challenges of high-dimensional regression and potential solutions.

> Apply Principal Component Analysis (PCA) on numerical features (Tenure, MonthlyCharges, TotalCharges) to reduce dimensionality.

### 12. Ridge Regression

> Implement Ridge Regression using Churn as the target variable and Tenure, MonthlyCharges, TotalCharges, and additional customer demographic features as predictors.

> Identify the optimal lambda using cross-validation.

### 13. Lasso Regression

> Implement Lasso Regression with the same feature set as Ridge Regression.

> Discuss feature selection benefits and interpret the coefficients.

